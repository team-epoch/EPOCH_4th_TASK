# Week2 Reading Task


## **ðŸ“˜Â Title**

> â„¹ï¸Â *APA. ì¸ìš© ë°©ì‹ ê¶Œê³ *
> Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, & K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems (Vol. 25, pp. 1097â€“1105). Curran Associates, Inc.

---


## **ðŸ“–Â Abstract**

> â„¹ï¸Â *ë³¸ì¸ì˜ ë°©ì‹ìœ¼ë¡œ ìž¬í•´ì„ í•´ì£¼ì„¸ìš”. ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆí•©ë‹ˆë‹¤.*
> CNNì„ ëŒ€ê·œëª¨ ì´ë¯¸ì§€ datasetì— ì„±ê³µì ìœ¼ë¡œ í•™ìŠµ ì‹œì¼°ë‹¤. ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ReLUì™€ GPUë¥¼ ë³‘ë ¬ì‹œì¼°ìœ¼ë©°, FC layerì˜ overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ "dropout" ë°©ë²•ì„ ì‚¬ìš©í–ˆë‹¤.
> ê²°ê³¼ì ìœ¼ë¡œ ì´ì „ì˜ SOTAë³´ë‹¤ í›¨ì”¬ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.

---


## **ðŸ“šÂ Background**

> â„¹ï¸Â *ë…¼ë¬¸ì˜ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬ë“¤ ë° ë°°ê²½ ì§€ì‹ì„ ì •ë¦¬*
> 
> 
> **ðŸ“Â Related Work 1**
> ê¸°ì¡´ì—ëŠ” activation functionìœ¼ë¡œ sigmoidì™€ tanhë¥¼ ì£¼ë¡œ ì‚¬ìš©í–ˆìœ¼ë‚˜, ì´ í•¨ìˆ˜ë“¤ì€ gradient vanishing ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ëŠ” saturating í•¨ìˆ˜ì˜€ìœ¼ë©° í•™ìŠµì´ ëŠë¦¬ë‹¤ëŠ” ë‹¨ì ì´ ìžˆì—ˆìŒ
> --> **ReLU(Nair & Hinton (2010))**: non-saturating í•¨ìˆ˜, ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥
> 
> **ðŸ“Â Related Work 2**
> **Dropout(Hinton et al. (2012))**: overfitting ë°©ì§€, ëžœë¤í•˜ê²Œ ë‰´ëŸ°ì„ ë¹„í™œì„±í™”í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ ê°•í™”

---


## **ðŸ”Â Methods**

> **âœ…Â ì‚¬ìš©ëœ ì—°êµ¬ ë°©ë²•**
> 1. 5 conv layers + 3 fc layers
> 2. ReLU ì‚¬ìš©(ëª¨ë“  layerì—)
> 3. 2ê°œì˜ GPU ë³‘ë ¬ í•™ìŠµ(GPUê°€ íŠ¹ì • layerì—ì„œë§Œ ì„œë¡œ communicate) -> ëŒ€ê·œëª¨ í•™ìŠµ ê°€ëŠ¥
> 4. Local Response Normalization: ê°™ì€ ìœ„ì¹˜ì—ì„œ ë‹¤ë¥¸ ì»¤ë„ì´ ë‚¸ activationì„ ê²½ìŸì‹œí‚¤ë©° generalization ê°œì„ (í‰ê· ì„ ë¹¼ì§€ ì•Šê³  ì¸ì ‘ ë‰´ëŸ°ë“¤ì˜ í¬ê¸°ì— ë”°ë¼ ë‚˜ëˆ ì„œ ì •ê·œí™”)
> 5. Overlapping Pooling(stride<kernel size): edgeì— ìžˆëŠ” ì¤‘ìš”í•œ ì •ë³´ë„ ë³´ì¡´í•˜ë©´ì„œ featureê°„ ê²½ìŸì„ ìœ ë„í•´ ì¤‘ìš”í•œ ì •ë³´ë§Œ ë‚¨ê¹€ -> overfitting ì–µì œ
> 6. Dropout ì •ê·œí™” ì ìš©
>
> 
> **âœ…Â ì‹¤í—˜ ì„¤ê³„**
> 1. Dataset: ImageNet ILSVRC-2010
> 2. Data Augmentation: random crop(input size 224x224ì— ë§žì¶°) + horizontal reflection + PCA on the set of RGB pixel
> 3. Details of learning setting: SGD + momentum, weight decay, mini-batch size 128
> 4. Evaluation: Top-1 / Top-5 error rate
>    
> **ðŸ“Â ëª¨ë¸ ë¹„êµ** 
> 1. SIFT + FVs(Traditional Method): Top-5 error=26.2%
> 2. 1 CNN(AlexNet): Top-5 error=18.2%
> 3. 5 CNNs(Ensemble): Top-5 error=16.4%
> 4. 7 CNNs* (Pretrain + Ensemble): **Top-5 error=15.3%** --> ìµœì¢… AlexNetì˜ ì„±ëŠ¥

---


## **ðŸ”Â Experiments**

> **âœ…Â ë°ì´í„°ì…‹**
> ImageNet ILSVRC-2010
> 
> **âœ…Â Models**
> 1. SIFT + FVs(Traditional Method)
> 2. 1 CNN(AlexNet)
> 3. 5 CNNs(Ensemble)
> 4. 7 CNNs* (Pretrain + Ensemble)
> 
> **âœ…Â Evaluation Metrics**
> Top-1 / Top-5 error rate
> 
> **âœ…Â Implementation Details**
> SGD + momentum 0.9, weight decay 0.0005, mini-batch size 128, lr(ì´ˆê¸°: 0.01, ì„±ëŠ¥ ì •ì²´ì‹œ 1/10ë¡œ ê°ì†Œ) 

---


## **ðŸ“–Â Conclusion**

> **âœ…Â Limitation**
> ëª¨ë¸ í¬ê¸°ê°€ ë„ˆë¬´ ë¬´ê²ê³ , íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ë§ŽìŒ
> GPU ì—†ì´ëŠ” ì¶”ë¡  ì¡°ì°¨ ë¶ˆê°€ëŠ¥í•¨
> 
> **âœ…Â Contribution**
> 1. ReLUì˜ ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥
> 2. FC layerì— Dropout ê¸°ë²•ì„ ì ìš©í•´ overfittingì„ í•´ê²°
> 3. GPU ë³‘ë ¬ ì—°ì‚°ìœ¼ë¡œ ëŒ€ê·œëª¨ CNN í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•¨

---


## **ðŸ¤”Â Question**

> â„¹ï¸Â *ë³¸ì¸ì´ ìˆ˜í–‰í•œ í•™ìŠµì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µí•´ë³´ì„¸ìš”.*
> 
> 
> **ðŸ“Â ì´ ë…¼ë¬¸ì´ ë“±ìž¥í•˜ê²Œ ëœ ì´ìœ  + ì´ ë…¼ë¬¸ì´ ê´€ë ¨ Taskì— ê¸°ì—¬í•œ ë‚´ìš©**
> ëŒ€ê·œëª¨ CNN í•™ìŠµì„ ê°€ëŠ¥í•˜ë„ë¡ í•˜ëŠ” GPU ë³‘ë ¬ ì—°ì‚°--> GPU ë”¥ëŸ¬ë‹ ì‹œëŒ€ì˜ ê°œë§‰...
> 
> **ðŸ“Â ë°°ìš¸ ìˆ˜ ìžˆì—ˆë˜ ë‚´ìš©ê³¼ ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì **
> Ensembleê³¼ pretrainingìœ¼ë¡œ ìµœì¢… Top-5 error=15.3%ë¼ëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆëŠ”ë° Ensembleê³¼ Pretrainingì˜ ë°©ë²•ì´ ë‹¬ëžë‹¤ë©´ ì„±ëŠ¥ì´ ë” ë†’ì•„ì¡Œì„ê¹Œ?
> 
