# Week3 Reading Task


## **ðŸ“˜Â Title**

> â„¹ï¸Â *APA. ì¸ìš© ë°©ì‹ ê¶Œê³ *
> He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770â€“778.

---


## **ðŸ“–Â Abstract**

> â„¹ï¸Â *ë³¸ì¸ì˜ ë°©ì‹ìœ¼ë¡œ ìž¬í•´ì„ í•´ì£¼ì„¸ìš”. ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆí•©ë‹ˆë‹¤.*
> ì£¼ìš” ë¬¸ì œì˜ì‹: very deep plain networkì—ì„œ depth ì¦ê°€ ì‹œ training error ì—­ì „ ì¦ê°€í•˜ëŠ” degradation problem ê´€ì°°ë¨.
> í•µì‹¬ ì•„ì´ë””ì–´: target mapping H(x)ë¥¼ ì§ì ‘ í•™ìŠµí•˜ì§€ ì•Šê³  residual F(x)=H(x)âˆ’xë¥¼ í•™ìŠµ. outputì„ y=F(x)+xë¡œ êµ¬ì„±. identity shortcutë¡œ parameter ì¦ê°€ ì—†ì´ gradient íë¦„ ê°œì„ . ì°¨ì› ë¶ˆì¼ì¹˜ ì‹œ projection shortcut ì‚¬ìš©.
> ê²°ê³¼: ImageNetì—ì„œ 50/101/152-layer ResNetì´ 18/34-layer ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ. single-model 152-layer top-5 val 4.49%. ensemble top-5 test 3.57%. ILSVRC 2015 1ìœ„.
> 

---


## **ðŸ“šÂ Background**

> â„¹ï¸Â *ë…¼ë¬¸ì˜ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬ë“¤ ë° ë°°ê²½ ì§€ì‹ì„ ì •ë¦¬*
> 
> 
> **ðŸ“Â Related Work 1**
> depth í™•ëŒ€ê°€ ì„±ëŠ¥ì— ì¤‘ìš”í•˜ë‚˜ plain networkëŠ” depth ì¦ê°€ ì‹œ optimization ë‚œì´ë„ ìƒìŠ¹ìœ¼ë¡œ training error ì¦ê°€. BN ì‚¬ìš©í•´ë„ degradation problem ì¡´ìž¬í•¨.
> **ðŸ“Â Related Work 2**
> shortcut ê¸°ë°˜ êµ¬ì¡°ê°€ í•™ìŠµì„ ìš©ì´í•˜ê²Œ í•  ìˆ˜ ìžˆìŒ. identity shortcutì´ parameter ì—†ì´ ìž‘ë™. projection shortcutì€ ì°¨ì› ë§¤ì¹­ìš©ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥.

---


## **ðŸ”Â Methods**

> **âœ…Â ì‚¬ìš©ëœ ì—°êµ¬ ë°©ë²•**
> residual learning ì±„íƒ. block ë‹¨ìœ„ë¡œ y=F(x,{W_i})+x êµ¬ì„±.
> FëŠ” 2-layer ë˜ëŠ” 3-layer convë¡œ êµ¬í˜„. ë™ì¼ ì°¨ì›ì€ identity shortcut.
> ì°¨ì› ë³€ê²½ì€ 1Ã—1 projection shortcutë¡œ ë§¤ì¹­. addition í›„ í™œì„±í•¨ìˆ˜ ì ìš©.
> 
> **âœ…Â ì‹¤í—˜ ì„¤ê³„**
> ImageNetìš©ìœ¼ë¡œ bottleneck block ë„ìž….
> 1Ã—1-3Ã—3-1Ã—1ë¡œ ì±„ë„ ì¶•ì†Œ-ì—°ì‚°-í™•ìž¥.
> identity shortcut ì‚¬ìš© ì‹œ ì—°ì‚°ëŸ‰ê³¼ íŒŒë¼ë¯¸í„° íš¨ìœ¨ ìœ ì§€.
> projectionì„ ê³¼ë„í•˜ê²Œ ì“°ë©´ ë¹„ìš© ì¦ê°€
> **ðŸ“Â ëª¨ë¸ ë¹„êµ** 
> plain-18/34 vs ResNet-18/34ë¡œ degradation ìœ ë¬´ ë¹„êµ.
> ì´í›„ ResNet-50/101/152ë¡œ depth scaling.
> ResNetì´ ì¼ê´€ë˜ê²Œ ìš°ì„¸.

---


## **ðŸ”Â Experiments**

> **âœ…Â ë°ì´í„°ì…‹**
> ImageNet ILSVRC 2012. train 1.28M. val 50k. test 100k. top-1, top-5 errorë¡œ í‰ê°€.
> CIFAR-10. train 50k. test 10k. ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ë§¤ìš° ê¹Šì€ network ë¶„ì„
> **âœ…Â Models**
> ImageNet: 18/34 plainê³¼ ëŒ€ì‘ ResNet. bottleneckìœ¼ë¡œ 50/101/152 êµ¬ì„±
> CIFAR-10: 6n+2 layer ì„¤ê³„. shortcutì€ identity ì‚¬ìš©. nì€ {3,5,7,9,18}. ìµœëŒ€ 1202-layer ì‹¤í—˜.
> **âœ…Â Evaluation Metrics**
> classification: top-1, top-5 error. detection: mAP@.5, mAP@[.5:.95].
> **âœ…Â Implementation Details**
> ImageNet:
> - scale jittering 256â€“480.
> - 224 crop. color augmentation.
> - BNì€ conv ë’¤ activation ì „.
> - SGD batch 256. lr 0.1ì—ì„œ ì‹œìž‘ í›„ plateauë§ˆë‹¤ /10.
> - total 60Ã—10^4 iters. weight decay 1e-4.
> - momentum 0.9.
> - dropout ë¯¸ì‚¬ìš©.
> - testëŠ” 10-crop ë˜ëŠ” fully-conv multi-scale.
> CIFAR-10:
> - batch 128.
> - lr 0.1.
> - schedule 32k, 48k decay.
> - 64k stop.
> - 4-pixel pad í›„ 32Ã—32 random crop.
> - flip.
> - dropout ì—†ìŒ.
> - 110-layerëŠ” warm-up 0.01 ì‚¬ìš© í›„ 0.1 ë³µê·€. 

---


## **ðŸ“–Â Conclusion**

> **âœ…Â Limitation**
> CIFAR-10ì—ì„œ 1202-layerëŠ” 110-layerë³´ë‹¤ test error ë†’ìŒ.
> overfitting ì¶”ì •.
> ê°•í•œ regularization ê²°í•© í•„ìš”ì„± ì–¸ê¸‰
> 
> **âœ…Â Contribution**
> residual learning ê³µì‹ì„ block ë‹¨ìœ„ë¡œ ì •ì‹í™”.
> identity shortcutë¡œ parameter ì¦ê°€ ì—†ì´ optimization ìš©ì´í™”.
> bottleneckìœ¼ë¡œ depthì™€ íš¨ìœ¨ ë™ì‹œ í™•ë³´.
> very deep networkì—ì„œ degradation ë¬¸ì œ í•´ì†Œ.
> ImageNet SOTA ë° detection ì „ì´ ì„±ê³¼ ë‹¬ì„±.

---


## **ðŸ¤”Â Question**

> â„¹ï¸Â *ë³¸ì¸ì´ ìˆ˜í–‰í•œ í•™ìŠµì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µí•´ë³´ì„¸ìš”.*
> extremely deepì—ì„œ generalization ì €í•˜ë¥¼ ë§‰ëŠ” regularization ì¡°í•© ìµœì í•´ ë¬´ì—‡?
> projection shortcut ë¹„ìœ¨ê³¼ ì„±ëŠ¥-íš¨ìœ¨ trade-off ìµœì ì ?
> detectionì—ì„œ BN freezing ëŒ€ì‹  ë‹¤ë¥¸ normalization ì„ íƒ?
> 
