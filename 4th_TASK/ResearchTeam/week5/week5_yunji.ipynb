{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnV19hzcZ1P",
        "outputId": "0fea5b24-6a16-4ed0-d463-4c96d7e0c82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.8.0+cu126\n",
            "--2025-09-23 02:55:38--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8143096 (7.8M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.77M  4.47MB/s    in 1.7s    \n",
            "\n",
            "2025-09-23 02:55:40 (4.47 MB/s) - ‘fra-eng.zip’ saved [8143096/8143096]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "Data preparation complete.\n",
            "French vocab size: 25080\n",
            "English vocab size: 15633\n",
            "DataLoader is ready.\n",
            "\n",
            "--- Final Model Test Start ---\n",
            "Using device: cuda\n",
            "\n",
            "--- TEST RESULT: SUCCESS! ---\n",
            "Source batch shape: torch.Size([64, 15])\n",
            "Target input shape: torch.Size([64, 13])\n",
            "Source mask shape: torch.Size([64, 1, 1, 15])\n",
            "Target mask shape: torch.Size([64, 1, 13, 13])\n",
            "Final output shape: torch.Size([64, 13, 15633])\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PART 1: 모델 정의 (Transformer 전체 아키텍처)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super(ResidualConnection, self).__init__()\n",
        "        self.norm = LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_head == 0\n",
        "        self.d_k = d_model // n_head\n",
        "        self.n_head = n_head\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_concat = nn.Linear(d_model, d_model)\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        q = self.w_q(q).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(k).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(v).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            # mask==0인 부분, 즉 <pad>이거나 미래 시점인 부분을 -1e9로 채움\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_k)\n",
        "        output = self.w_concat(context)\n",
        "        return output\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.res_conn1 = ResidualConnection(d_model, dropout)\n",
        "        self.res_conn2 = ResidualConnection(d_model, dropout)\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.res_conn1(x, lambda x: self.self_attn(x, x, x, src_mask))\n",
        "        x = self.res_conn2(x, self.feed_forward)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, d_ff, n_layers, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.norm = LayerNorm(d_model)\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, n_head)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.res_conn1 = ResidualConnection(d_model, dropout)\n",
        "        self.res_conn2 = ResidualConnection(d_model, dropout)\n",
        "        self.res_conn3 = ResidualConnection(d_model, dropout)\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.res_conn1(x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.res_conn2(x, lambda x: self.enc_dec_attn(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.res_conn3(x, self.feed_forward)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, d_ff, n_layers, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_head, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.norm = LayerNorm(d_model)\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_head, d_ff, n_layers, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, n_head, d_ff, n_layers, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, n_head, d_ff, n_layers, dropout)\n",
        "        self.generator = nn.Linear(d_model, tgt_vocab_size)\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "        return self.generator(decoder_output)\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 2: 데이터 준비 (Tatoeba 데이터셋)\n",
        "# ==============================================================================\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 데이터셋 다운로드 및 압축 해제\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip -O fra-eng.zip\n",
        "!unzip -o fra-eng.zip\n",
        "\n",
        "PAD_TOKEN = 0\n",
        "SOS_TOKEN = 1\n",
        "EOS_TOKEN = 2\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.index2word = {PAD_TOKEN: \"<pad>\", SOS_TOKEN: \"<s>\", EOS_TOKEN: \"</s>\"}\n",
        "        self.n_words = 3\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower().strip()\n",
        "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def prepare_data():\n",
        "    lines = open('fra.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = [[normalize_string(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "    pairs = [list(reversed(p)) for p in pairs] # Reverse for French -> English\n",
        "    input_lang, output_lang = Vocabulary('fra'), Vocabulary('eng')\n",
        "    MAX_LENGTH = 15\n",
        "    pairs = [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH]\n",
        "    for pair in pairs:\n",
        "        input_lang.add_sentence(pair[0])\n",
        "        output_lang.add_sentence(pair[1])\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepare_data()\n",
        "print(\"Data preparation complete.\")\n",
        "print(f\"French vocab size: {input_lang.n_words}\")\n",
        "print(f\"English vocab size: {output_lang.n_words}\")\n",
        "\n",
        "def indexes_from_sentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "class TatoebaDataset(Dataset):\n",
        "    def __init__(self, pairs, input_lang, output_lang):\n",
        "        self.pairs = pairs\n",
        "        self.input_lang = input_lang\n",
        "        self.output_lang = output_lang\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "        input_tensor = torch.tensor(indexes_from_sentence(self.input_lang, pair[0]), dtype=torch.long)\n",
        "        output_tensor = torch.tensor(indexes_from_sentence(self.output_lang, pair[1]), dtype=torch.long)\n",
        "        return input_tensor, output_tensor\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(torch.cat([torch.tensor([SOS_TOKEN]), src_sample, torch.tensor([EOS_TOKEN])], dim=0))\n",
        "        tgt_batch.append(torch.cat([torch.tensor([SOS_TOKEN]), tgt_sample, torch.tensor([EOS_TOKEN])], dim=0))\n",
        "    src_padded = pad_sequence(src_batch, padding_value=PAD_TOKEN, batch_first=True)\n",
        "    tgt_padded = pad_sequence(tgt_batch, padding_value=PAD_TOKEN, batch_first=True)\n",
        "    return src_padded, tgt_padded\n",
        "\n",
        "dataset = TatoebaDataset(pairs, input_lang, output_lang)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "print(\"DataLoader is ready.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 3: 최종 마스크 생성 함수 및 모델 테스트\n",
        "# ==============================================================================\n",
        "\n",
        "def create_masks(src, tgt, device):\n",
        "    \"\"\"\n",
        "    소스 및 타겟 시퀀스에 대한 마스크를 생성합니다.\n",
        "    이 함수는 브로드캐스팅을 활용하여 올바른 4D 마스크를 생성합니다.\n",
        "    \"\"\"\n",
        "    # 소스 마스크 (B, 1, 1, S_len)\n",
        "    src_mask = (src != PAD_TOKEN).unsqueeze(1).unsqueeze(2).to(device)\n",
        "\n",
        "    # 타겟 마스크 (B, 1, T_len, T_len)\n",
        "    seq_len = tgt.size(1)\n",
        "    # 패딩 마스크 (B, 1, T_len, 1)\n",
        "    tgt_pad_mask = (tgt != PAD_TOKEN).unsqueeze(1).unsqueeze(3).to(device)\n",
        "    # 룩어헤드 마스크 (T_len, T_len)\n",
        "    tgt_look_ahead_mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).bool()\n",
        "    # 최종 결합\n",
        "    tgt_mask = tgt_pad_mask & tgt_look_ahead_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# --- 테스트 실행 ---\n",
        "print(\"\\n--- Final Model Test Start ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 하이퍼파라미터\n",
        "D_MODEL = 512\n",
        "N_HEAD = 8\n",
        "D_FF = 2048\n",
        "N_LAYERS = 6\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# 모델 생성\n",
        "model = Transformer(\n",
        "    src_vocab_size=input_lang.n_words,\n",
        "    tgt_vocab_size=output_lang.n_words,\n",
        "    d_model=D_MODEL,\n",
        "    n_head=N_HEAD,\n",
        "    d_ff=D_FF,\n",
        "    n_layers=N_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "# 데이터로더에서 한 배치 가져오기\n",
        "src_batch, tgt_batch = next(iter(dataloader))\n",
        "src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
        "\n",
        "tgt_input = tgt_batch[:, :-1]\n",
        "\n",
        "# 마스크 생성\n",
        "src_mask, tgt_mask = create_masks(src_batch, tgt_input, device)\n",
        "\n",
        "# 모델 forward pass\n",
        "try:\n",
        "    output = model(src_batch, tgt_input, src_mask, tgt_mask)\n",
        "    print(\"\\n--- TEST RESULT: SUCCESS! ---\")\n",
        "    print(f\"Source batch shape: {src_batch.shape}\")\n",
        "    print(f\"Target input shape: {tgt_input.shape}\")\n",
        "    print(f\"Source mask shape: {src_mask.shape}\")\n",
        "    print(f\"Target mask shape: {tgt_mask.shape}\")\n",
        "    print(f\"Final output shape: {output.shape}\")\n",
        "except Exception as e:\n",
        "    print(\"\\n--- TEST RESULT: FAILED ---\")\n",
        "    print(f\"An error occurred: {e}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\" 1 에폭 동안 모델을 훈련시키는 함수 \"\"\"\n",
        "    model.train()  # 모델을 훈련 모드로 설정\n",
        "    total_loss = 0\n",
        "\n",
        "    for src_batch, tgt_batch in dataloader:\n",
        "        src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
        "\n",
        "        # 훈련 데이터 준비\n",
        "        # 디코더 입력: <s>부터 마지막에서 두 번째 단어까지\n",
        "        tgt_input = tgt_batch[:, :-1]\n",
        "        # 정답 레이블: 첫 번째 단어 <s>를 제외하고, </s>까지\n",
        "        tgt_output = tgt_batch[:, 1:]\n",
        "\n",
        "        # 마스크 생성\n",
        "        src_mask, tgt_mask = create_masks(src_batch, tgt_input, device)\n",
        "\n",
        "        # 순전파 및 역전파\n",
        "        optimizer.zero_grad() # 그래디언트 초기화\n",
        "        prediction = model(src_batch, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "        # 손실 계산\n",
        "        # prediction: (B, T, V) -> (B * T, V)\n",
        "        # tgt_output: (B, T) -> (B * T)\n",
        "        loss = criterion(\n",
        "            prediction.reshape(-1, prediction.size(-1)),\n",
        "            tgt_output.reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward() # 역전파\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # 그래디언트 클리핑 (학습 안정화)\n",
        "        optimizer.step() # 파라미터 업데이트\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\" 모델의 성능을 평가하는 함수 \"\"\"\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad(): # 그래디언트 계산 비활성화\n",
        "        for src_batch, tgt_batch in dataloader:\n",
        "            src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
        "\n",
        "            tgt_input = tgt_batch[:, :-1]\n",
        "            tgt_output = tgt_batch[:, 1:]\n",
        "\n",
        "            src_mask, tgt_mask = create_masks(src_batch, tgt_input, device)\n",
        "\n",
        "            prediction = model(src_batch, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "            loss = criterion(\n",
        "                prediction.reshape(-1, prediction.size(-1)),\n",
        "                tgt_output.reshape(-1)\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# --- 메인 훈련 루프 ---\n",
        "N_EPOCHS = 10\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# Loss 함수와 Optimizer 정의\n",
        "# PAD_TOKEN은 Loss 계산에서 무시하도록 설정\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 훈련 및 평가\n",
        "    train_loss = train_epoch(model, dataloader, optimizer, criterion, device)\n",
        "    valid_loss = evaluate(model, dataloader, criterion, device) # 간단히 훈련 데이터로 평가\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f}\")\n",
        "    print(f\"\\t Val. Loss: {valid_loss:.3f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-XJtdo6dG70",
        "outputId": "7631f40f-9f97-4cfc-fcd6-495ee882fdb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training ---\n",
            "Epoch: 01 | Time: 10m 15s\n",
            "\tTrain Loss: 3.160\n",
            "\t Val. Loss: 2.361\n"
          ]
        }
      ]
    }
  ]
}