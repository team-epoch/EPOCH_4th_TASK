# Week2 Reading Task


## **ðŸ“˜Â Title**

> â„¹ï¸Â *APA. ì¸ìš© ë°©ì‹ ê¶Œê³ *
> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need.Â Advances in neural information processing systems,Â 30.

---


## **ðŸ“–Â Abstract**

> â„¹ï¸Â *ë³¸ì¸ì˜ ë°©ì‹ìœ¼ë¡œ ìž¬í•´ì„ í•´ì£¼ì„¸ìš”. ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆí•©ë‹ˆë‹¤.*
> ê¸°ì¡´ì˜ convolutionì„ ê¸°ë°˜ìœ¼ë¡œ sequence transduction ëª¨ë¸ê³¼ ë‹¬ë¦¬ Transformerë¼ëŠ” ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬
  attention ë§¤ì»¤ë‹ˆì¦˜ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤. ê¸°ì¡´ì˜ SOTA ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

---


## **ðŸ“šÂ Background**

> â„¹ï¸Â *ë…¼ë¬¸ì˜ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬ë“¤ ë° ë°°ê²½ ì§€ì‹ì„ ì •ë¦¬*
> 
> 
> **ðŸ“Â Related Work 1**
> **RNN, CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ sequential computation**: RNNì€ ë³‘ë ¬í™”ê°€ ë¶ˆê°€ëŠ¥, CNNì€ ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•˜ë‚˜ ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ ì‚¬ì´ì˜ dependencyë¥¼ í•™ìŠµí•˜ëŠ”ë° í•„ìš”í•œ ì—°ì‚°ì´ ì»¤ì§
> **ðŸ“Â Related Work 2**
> **Self-attention**: ë¬¸ìž¥ ì•ˆì˜ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì§ì ‘ ì—°ê²°í•  ìˆ˜ ìžˆì–´ ê±°ë¦¬ì™€ ë¬´ê´€í•˜ê²Œ dependencyë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìžˆë‹¤.  

---


## **ðŸ”Â Methods**

> **âœ…Â ì‚¬ìš©ëœ ì—°êµ¬ ë°©ë²•**
> 1. ê¸°ì¡´ RNN/LSTM, CNN ê¸°ë°˜ êµ¬ì¡°ë¥¼ ëª¨ë‘ ë°°ì œí•˜ê³ , ì „ì ìœ¼ë¡œ Self-Attentionì— ê¸°ë°˜í•œ Transformer ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•¨
> 2. ì¸ì½”ë”â€“ë””ì½”ë” êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë˜, ê° ì¸µì„ Multi-Head Self-Attention + Feed-Forward Networkë¡œ êµ¬ì„±í•¨
> 3. ìœ„ì¹˜ ì •ë³´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ Positional Encoding ë„ìž…
> 
> **âœ…Â ì‹¤í—˜ ì„¤ê³„**
> 1. WMT 2014 Englishâ€“German, Englishâ€“Frenchë¥¼ ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ë¡œ ì„¤ì •
> 2. Base ëª¨ë¸(N=6, d_model=512, 8 heads) vs. Big ëª¨ë¸(N=6, d_model=1024, 16 heads)
> 3. 8 Ã— NVIDIA P100 GPU, 3.5ì¼ í•™ìŠµ
> 4. í‰ê°€ ì§€í‘œ: BLEU score
>    
> **ðŸ“Â ëª¨ë¸ ë¹„êµ** 
> ê¸°ì¡´ RNN/CNN ê¸°ë°˜ ëª¨ë¸ vs. Transformer

---


## **ðŸ”Â Experiments**

> **âœ…Â ë°ì´í„°ì…‹**
> WMT 2014 Englishâ€“German(ì•½ 450ë§Œ ë¬¸ìž¥), WMT 2014 Englishâ€“French(ì•½ 3600ë§Œ ë¬¸ìž¥), English Constituency Parsing(ì¶”ê°€ ê²€ì¦ìš©)
> 
> **âœ…Â Models**
> 1. Transformer(Base, Big)
> 2. RNN ê¸°ë°˜ ëª¨ë¸
> 3. CNN ê¸°ë°˜ ëª¨ë¸
>    
> **âœ…Â Evaluation Metrics**
> BLEU score
> 
> **âœ…Â Implementation Details**
> 1. Optimization: Adam optimizer, learning rate warm-up, decay schedule
> 2. Regularization: Dropout, Label Smoothing (Îµ = 0.1)
> 3. 8 Ã— NVIDIA P100 GPU
> 4. í† í°í™”: Byte-Pair Encoding (BPE) ê¸°ë°˜ ë‹¨ì–´ ë¶„ë¦¬

---


## **ðŸ“–Â Conclusion**

> **âœ…Â Limitation**
> 1. ë‹¹ì‹œì—ëŠ” ì£¼ë¡œ ê¸°ê³„ ë²ˆì—­(MT) ì¤‘ì‹¬ìœ¼ë¡œ ê²€ì¦ë¨
> 2. ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ìžì›ì´ í•„ìš”í•˜ë‹¤.
> 3. Self-Attentionì€ ëª¨ë“  í† í° ìŒì„ ê³ ë ¤í•´ì•¼ í•˜ë¯€ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—°ì‚°ëŸ‰ì´ ì¦ê°€
>    
> **âœ…Â Contribution**
> 1. RNNê³¼ CNNì„ ë°°ì œí•˜ê³  ì˜¤ë¡œì§€ Self-attentionë§Œì„ ì‚¬ìš©í•´ êµ¬ì„±í•œ ìµœì´ˆì˜ sequence transduction ëª¨ë¸ì´ì—ˆë‹¤.
> 2. ê¸°ì¡´ RNNê³¼ CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ ëŒ€ë¹„ í›¨ì”¬ ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„ì™€ ë” í° batch ì²˜ë¦¬ ê°€ëŠ¥í•´
> 3. SOTA ë‹¬ì„±
> 4. ë²ˆì—­ ì™¸ì—ë„ ë‹¤ë¥¸ NLP ê³¼ì œì—ë„ ì ìš© ê°€ëŠ¥

---


## **ðŸ¤”Â Question**

> â„¹ï¸Â *ë³¸ì¸ì´ ìˆ˜í–‰í•œ í•™ìŠµì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µí•´ë³´ì„¸ìš”.*
> 
> 
> **ðŸ“Â ì´ ë…¼ë¬¸ì´ ë“±ìž¥í•˜ê²Œ ëœ ì´ìœ  + ì´ ë…¼ë¬¸ì´ ê´€ë ¨ Taskì— ê¸°ì—¬í•œ ë‚´ìš©**
> ê¸°ì¡´ RNN/LSTM ê¸°ë°˜ ëª¨ë¸ì€ ìˆœì°¨ì  ì—°ì‚°ì„ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë³‘ë ¬í™”ê°€ ë¶ˆê°€ëŠ¥í–ˆê³ , ê¸´ sequenceì—ì„œ dependecny í•™ìŠµì— í•œê³„ê°€ ìžˆì—ˆë‹¤.
> ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•œ CNN ê¸°ë°˜ ëª¨ë¸ë„ ë“±ìž¥í–ˆì§€ë§Œ, ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ í•™ìŠµì´ ë¹„íš¨ìœ¨ì ì´ì—ˆë‹¤.
> ê¸°ì¡´ì˜ RNNê³¼ CNNì„ ëª¨ë‘ ì œê±°í•˜ê³ , ì˜¤ì§ "Self-Attention"ë§Œìœ¼ë¡œ êµ¬ì„±ëœ ìµœì´ˆì˜ sequence ëª¨ë¸ì„ ì œì•ˆí–ˆê³ , ê¸°ê³„ ë²ˆì—­ì—ì„œ ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ì„ í¬ê²Œ ë›°ì–´ë„˜ìœ¼ë©´ì„œ ë²ˆì—­ í’ˆì§ˆê³¼ ë³‘ë ¬í™”, í•™ìŠµ íš¨ìœ¨ì„ ë™ì‹œì— ë‹¬ì„±í–ˆë‹¤.
>
> **ðŸ“Â ë°°ìš¸ ìˆ˜ ìžˆì—ˆë˜ ë‚´ìš©ê³¼ ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì **
> self-attentionì˜ ì—°ì‚°ëŸ‰ ë¬¸ì œë¥¼ í•´ê²°í•œ ì´í›„ ì—°êµ¬ê°€ ìžˆëŠ”ì§€? ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í•´ê²°í–ˆëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤.
> Transformerê°€ NLP ë¶„ì•¼ ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ì˜ì—­ì—ë„ ì‚¬ìš©ë˜ê³  ìžˆëŠ”ë° êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ í™•ìž¥ë  ìˆ˜ ìžˆì—ˆëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤.
> 
