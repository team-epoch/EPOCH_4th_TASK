# Week4 Reading Task


## **ðŸ“˜Â Title**

> â„¹ï¸Â *APA. ì¸ìš© ë°©ì‹ ê¶Œê³ *   
 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
>

---


## **ðŸ“–Â Abstract**

> â„¹ï¸Â *ë³¸ì¸ì˜ ë°©ì‹ìœ¼ë¡œ ìž¬í•´ì„ í•´ì£¼ì„¸ìš”. ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆí•©ë‹ˆë‹¤.*   
2017ë…„, Recurrent, Convolutionì„ ì™„ì „ížˆ ë°°ì œí•˜ê³  Attentionì—ë§Œ ì˜ì¡´í•˜ëŠ” ì‹ ê²½ë§ Transformerì´ ë“±ìž¥í–ˆë‹¤. ì´ ëª¨ë¸ì˜ ë“±ìž¥ ì „í›„ë¡œ ìžì—°ì–´ì²˜ë¦¬ì˜ ì–‘ìƒì€ ì™„ì „ížˆ ë‹¬ë¼ì¡Œìœ¼ë©°, í˜„ìž¬ ìš°ë¦¬ê°€ íŽ¸í•˜ê²Œ ì‚¬ìš©í•˜ëŠ” ChatGPT ë“± ìƒì„±í˜•AIì—­ì‹œ Transformer ê¸°ë°˜ì´ë‹¤. ìžì—°ì–´ì²˜ë¦¬ ë¿ ì•„ë‹ˆë¼, CV, Multimodal ë“± ìˆ˜ë§Žì€ AIë¶„ì•¼ì—ì„œ ì§€ê¸ˆê¹Œì§€ í™œìš©ë˜ê³  ìžˆìœ¼ë©°, ìˆ˜ë§Žì€ ë¶„ì•¼ì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆë‹¤.
>

---


## **ðŸ“šÂ Background**

> â„¹ï¸Â *ë…¼ë¬¸ì˜ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬ë“¤ ë° ë°°ê²½ ì§€ì‹ì„ ì •ë¦¬*
> 
> 
> **ðŸ“Â Related Work 1**   
Sequence-to-Sequence : Encoder, Decoderë¼ëŠ” ë‘ ê°œì˜ RNNìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê¸°ê³„ë²ˆì—­, ìš”ì•½, ì±—ë´‡ ë“± ë‹¤ì–‘í•œ NLPì— ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì´ë‹¤. ì´ˆê¸° seq2seq ëª¨ë¸ì€ ê¸´ ë¬¸ìž¥ì„ í•˜ë‚˜ì˜ ê³ ì •ëœ Context Vectorë¡œ ì••ì¶•í•´ì•¼ í–ˆê¸° ë•Œë¬¸ì—, ë¬¸ìž¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì •ë³´ê°€ ì†ì‹¤ë˜ì—ˆë‹¤.
> 
> **ðŸ“Â Related Work 2**
>LSTM : RNNì˜ long-term dependenciesë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ RNNì˜ í•œ ì¢…ë¥˜ë‹¤. ì¼ë°˜ì ì¸ RNNì€ ë¬¸ìž¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì´ˆë°˜ë¶€ì˜ ì •ë³´ê°€ í¬ë¯¸í•´ì ¸ê°€ëŠ” Vanishing Gradientê°€ ë°œìƒí•˜ì§€ë§Œ, LSTMì€ ì´ë¥¼ ê·¹ë³µí•˜ê³  ë¨¼ ê³¼ê±°ì˜ ì •ë³´ë„ ê¸°ì–µí•  ìˆ˜ ìžˆë‹¤.

---


## **ðŸ”Â Methods**

> **âœ…Â ì‚¬ìš©ëœ ì—°êµ¬ ë°©ë²•**   
> RNNê³¼ CNNì„ ì™„ì „ížˆ ì œê±°í•˜ê³ , Attentionì´ë¼ëŠ” ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œ Sequence ë³€í™˜ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì˜€ë‹¤.   
seq2seqëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Encoderì™€ Decoderë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‘˜ ë‹¤ Attention Blockê³¼ Position-wise Feed-Forward Networkë¡œ ì—°ê²°ëœë‹¤.   
RNNì²˜ëŸ¼ ìˆœí™˜ì ì¸ êµ¬ì¡°ëŠ” ì—†ì§€ë§Œ, Multi-Head Attentionì„ í†µí•´ Self Attentionì„ ì—¬ëŸ¬ ë²ˆ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ì— Attention í•˜ë„ë¡ í•˜ì˜€ë‹¤.
> 
> **âœ…Â ì‹¤í—˜ ì„¤ê³„**   
> English-to-German translation task   
Ecnglish Constituency Parsing task 
> **ðŸ“Â ëª¨ë¸ ë¹„êµ**   
GNMT (Googleâ€™s Neural Machine Translation, 2016)   
ConvS2S (Convolutional Sequence to Sequence, 2017)   
ê¸°ì¡´ attention-based RNN ëª¨ë¸ë“¤

---


## **ðŸ”Â Experiments**

> **âœ…Â ë°ì´í„°ì…‹**
> WMT 2014 Englishâ€“German translation task(4.5M)
WMT 2014 Englishâ€“French translation task (36M)
> 
> **âœ…Â Models**   
> Transformer   
> **âœ…Â Evaluation Metrics**   
> BLEU score   
> **âœ…Â Implementation Details**   
>Optimizer : Adam (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=1e-9)   
Warmup steps : 4000, step^0.5 decrease   
Regularization : Dropout rate 0.1   
8ê°œì˜ NVIDIA P100 GPU ì‚¬ìš©   
Base ëª¨ë¸: ì•½ 12ì‹œê°„ í•™ìŠµ   
Big ëª¨ë¸: ì•½ 3.5ì¼ í•™ìŠµ   


---


## **ðŸ“–Â Conclusion**

> **âœ…Â Limitation**   
>  íŠ¸ëžœìŠ¤í¬ë¨¸ëŠ” ë¬¸ìž¥ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ Attentionì˜ ê³„ì‚°ëŸ‰ì´ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ì œê³± O(n^2)ì— ë¹„ë¡€í•˜ì—¬ ê¸‰ì¦í•˜ê³ , ì´ëŠ” Bottleneckì„ ìœ ë°œí•œë‹¤.   
ìˆœì„œë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ìœ„ì¹˜ ì¸ì½”ë”©(Positional Encoding)ì— ì˜ì¡´í•˜ê³  ì´ëŠ” ìˆœì„œ ì •ë³´ê°€ ì¤‘ìš”í•œ ì¼ë¶€ ìž‘ì—…ì—ì„œ RNNë³´ë‹¤ ë¶ˆë¦¬í•  ìˆ˜ ìžˆë‹¤.   
> 
> **âœ…Â Contribution**   
>ìˆœì°¨ì ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” RNNê³¼ ë‹¬ë¦¬, ëª¨ë“  ë‹¨ì–´ë¥¼ ë™ì‹œì— ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìžˆê²Œ í•¨ìœ¼ë¡œì¨ í›ˆë ¨ ì†ë„ë¥¼ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œì¼°ë‹¤.
ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œë„ ë§¤ìš° ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìžˆìŒì„ ì¦ëª…í•˜ì˜€ê³ , ì´í›„ NLPì™¸ì—ë„ ê°ì¢… ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œê°€ ë˜ì—ˆë‹¤.

---


## **ðŸ¤”Â Question**

> â„¹ï¸Â *ë³¸ì¸ì´ ìˆ˜í–‰í•œ í•™ìŠµì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µí•´ë³´ì„¸ìš”.*
> 
> 
> **ðŸ“Â ì´ ë…¼ë¬¸ì´ ë“±ìž¥í•˜ê²Œ ëœ ì´ìœ  + ì´ ë…¼ë¬¸ì´ ê´€ë ¨ Taskì— ê¸°ì—¬í•œ ë‚´ìš©**   
CNN ê¸°ë°˜ ëª¨ë¸ì€ ë³‘ë ¬í™”ëŠ” ê°€ëŠ¥í–ˆì§€ë§Œ, ìž¥ê¸° ì˜ì¡´ì„±ì„ í¬ì°©í•˜ê¸° ìœ„í•´ì„œëŠ” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ê°€ í•„ìš”í–ˆë‹¤. ë˜í•œ ìˆœì°¨ì  êµ¬ì¡° ì—†ì´, ë³‘ë ¬ì ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê°„ì˜ ì˜ì¡´ì„±ì„ í•™ìŠµí•  í•„ìš”ê°€ ìžˆì—ˆë‹¤. Attention êµ¬ì¡°ë¥¼ í†µí•´ RNN/CNNì„ ëŒ€ì²´í•  ìˆ˜ ìžˆëŠ” ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•˜ì˜€ê³ , NLP ì™¸ì—ë„ í˜„ëŒ€ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì˜ ê¸°ë°˜ ì•„í‚¤í…ì²˜ê°€ ë˜ì—ˆë‹¤.
> 
> **ðŸ“Â ë°°ìš¸ ìˆ˜ ìžˆì—ˆë˜ ë‚´ìš©ê³¼ ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì **   
>í˜„ìž¬ ëŒ€ë¶€ë¶„ì˜ NLPì— Transformerë¥¼ ì‚¬ìš©í•˜ê³  ìžˆëŠ”ë°, ì•„ì§ NLP ë¶„ì•¼ ë‚´ì—ì„œ Transformerë³´ë‹¤ RNN/CNNì´ ìš°ì„¸í•œ ë¶„ì•¼ê°€ ìžˆëŠ”ì§€ ê¶ê¸ˆí–ˆë‹¤. ìž„ë² ë””ë“œ, IoTë“± ë©”ëª¨ë¦¬, ê³„ì‚°ëŸ‰ì´ ì œí•œì ì¸ í™˜ê²½ì—ì„  ì•„ì§ RNNì´ ë‹¤ì†Œ ìœ ë¦¬í•˜ë‹¤ê³  í•œë‹¤. (ì˜¨ë¼ì¸ ìŒì„±ì¸ì‹)
> 
